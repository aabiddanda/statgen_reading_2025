\documentclass{pset}
\name{Arjun Biddanda}
\date{6/1/2025}
\psnum{1}
\class{Statistical Genetics Reading Group}
\usepackage{verbatim}

\begin{document}
\maketitle

\section*{Probability Distributions (Review)}


\subsection*{Discrete Distributions}




\subsection*{Continuous Distribution}




\section*{Likelihoods and P-values}

\subsection*{Likelihood of Linear Regression}

Linear regression is a central model for relating predictor variables ($\mathbf{X}$) to response variables ($\mathbf{y}$). The $N$ ($x_i, y_i$) i.i.d observations are related through the following probability model: 

$$y_i \sim \mathcal{N}(x_i\beta, \sigma)$$ 

Using this definition:

\begin{itemize}
\item Write the expression for the log-likelihood of $N$ datapoints in terms of $\beta,\sigma$ using the normal density function.
\item Using the data provided in \texttt{data/pset1/linregA.tsv}, calculate the log-likelihood of the data with $\beta = 0,\sigma=1$. Calculate the log-likelihood at the value of $\hat{\beta}$ which maximizes the log-likelihood. 
\item For computational estimation, many approaches use ``least-squares'', which tries to find the $\beta$ which minimizes $\sum_i (y_i - x_i\beta)^2$. Calculate the least-squares estimate of $\beta$ based on the current data. Comment on how this is similar (both mathematically and empirically) to the maximum-likelihood estimator.
\end{itemize}

\subsection*{Mixture Models}

We want to model how p-values are distributed in an experiment, and from some early looks it appears that some portion of them appear to be non-uniformly distributed (like we would expect under the null). You choose to model each p-value as coming from a mixture of beta distributions:

$$\overrightarrow{\mathbf{p}} \overset{\text{iid}}{\sim} \pi_0 \text{Beta}(1, \beta) + (1 - \pi_0)\text{Beta}(1,1)$$, 

where the fraction $\pi_0$ can be treated as an estimate of the proportion of non-uniform p-values (e.g. proportion of rejections).

\begin{enumerate}
\item Write down the log-likelihood function for p-values (e.g. $\log (\mathbb{P}(\overrightarrow{\mathbf{p}} | \pi_0, \beta))$)
\item Using the data in \texttt{data/pset1/sumstats\_testA.tsv}, plot the log-likelihood as a function of $\pi_0$ when $\beta = 4$. Provide an estimate of the value of $\pi_0$ that maximizes the log-likelihood. (Hint: look up tricks to avoid numerical underflow.)
\item Plot the log-likelihood as a function of $\pi_0$ with $\beta = 4$ for the data in \texttt{data/pset1/sumstats\_testB.tsv}. How does the curve appear relative to the first dataset?
\item $\bLozenge\bLozenge$ The dataset also contains estimates of the standardized effect-size (or Z-score) corresponding to the p-values. Using the following mixture model for the Z-scores: 
$$\overrightarrow{\mathbf{Z}} \overset{\text{iid}}{\sim} \pi_0 \mathcal{N}(0,1) + (1 - \pi_0)\mathcal{N}(0,\sigma)$$

repeat the above steps to estimate $\hat{\pi_0}$ using the Z-scores. Comment on the estimated confidence intervals for $\hat{\pi_0}$ (using 2 log-likelihood units) between the effect-size and p-value mixture models (using $\beta=4,\sigma=3$ for example).

\end{enumerate}

\section*{Multiple Testing Correction and False Discovery Rates}

\subsection*{Comparing FWER and FDR}

There are two error rates relevant to statistical genetics, the family-wise error rate (FWER) and the false-discovery rate (FDR):

$$
\begin{aligned}
FWER &= P(R > 0 | H_0)\\
FDR &= P(R/n \leq r | H_0)\\
\end{aligned},
$$

where $R$ is a random variable representing a rejection of the null hypothesis and $n$ is the total number of tests. 

\begin{enumerate}
\item Assuming that you are conducting $n=10^6$ independent hypothesis tests, at a marginal testing threshold of $\alpha = 0.05$ what is the FWER and FDR respectively?  (Hint: consider the distribution of null p-values as Uniform and properties of the Bernoulli distribution to show this analytically).

\item At what marginal threshold of $\alpha$ does the FWER drop below $0.05$ for the first time? (relate this to the Bonferroni-threshold of $5\times 10^{-8}$)? 
\item Plot the FWER and FDR as a function of the marginal p-value threshold $\alpha$ and indicate the Bonferroni threshold on the plot

\end{enumerate}

\subsection*{False Discoveries in Empirical Data}

Using empirical GWAS data for height, calculate the number of total signals 

\section*{GWAS Power and Study Design}

\subsubsection*{Sample Size}

Suppose you are conducting a GWAS for quantitative trait $YFT$ (your favorite quantitative trait), and you want to collect $N=100,000$ individuals. 

\begin{itemize}
	\item Under an expected power of 0.8, what is the minimal effect size ($\beta$) detectable for a fully typed variant $r^2 = 1$ at an MAF of: 
	\begin{itemize}
		\item 5\% 
		\item 1\%
		\item 0.1\%
	\end{itemize}
	\item Your colleague wants to see if a specific missense variant at 0.1\% frequency in the population (with an imputation $r^2 = 0.8$), would be detectable in your current GWAS design?.What would be the minimal effect-size detectable for this variant?  
\end{itemize}

\subsubsection*{$\bLozenge\bLozenge$ GWAS Discovery Rate} 

\begin{enumerate}
\item Fixing GWAS power at $0.8$ and fully typed variants ($r^2 = 1)$, if causal variant effects are drawn from $\beta_{causal} \sim \mathcal{N}(0,\sigma^2)$, what is the \textit{expected fraction of additional discoveries} made in going from $N_1 = 10^4$ to $N_2 = 10^5$ when $\sigma^2 = 2$? (Assume causal allele frequencies are drawn from the Uniform distribution). 
	
\item If $\beta | p \sim N(0, \sigma^2)$, where $\sigma^2 = \left(p(1-p)\right)^{\alpha}$ for causal variants (e.g. the effect size is linked to allele frequency), what is the expected fraction of additional discoveries when $\alpha = 0.75$ (assume causal allele frequencies are drawn from the Uniform distribution).
\end{enumerate}

Provide either an analytical solution or a plot reflecting the numerical solution for both scenarios. 

\subsubsection*{$\bLozenge\bLozenge\bLozenge$  Effect of Tagging Variant LD}

The $r^2$ measure of LD is critical when you do not have direct access to a causal variant and only have a potential tagging variant. If we assume that $p_{tag} >= p_{causal}$, the expression is: 

$$r^2_{max}(tag, causal) = \frac{(1 - p_{tag})(1 - p_{causal})}{p_{tag}p_{causal}}$$

Using the expressions for GWAS power for a quantitative trait, and $r^2_{max}(p_{tag}, p_{causal})$, show that:

$$Power_{tag} \geq Power_{tag} \forall p_{tag} \geq p_{causal}$$

\subsubsection*{$\bLozenge\bLozenge$ Comparing GWAS}

Your colleague has an interesting case where $YFT$ is highly prevalent in a different population (lets say population ``B'')and wants to understand if the same variant is driving this increase in prevalence. Your collaborator has access to $N_B=50000$ samples, but doesn't have a good sense of whether this would be well-suited to address the idea.

If the effect-size of the causal variant is the same $\beta_A = \beta_B$, what would be the difference in frequency of the causal variant required to maintain a power of 80\% if the variant in population $A$ is at 0.1\% frequency? (Assume $N_A = 100,000$)  


\section*{GWAS Practical}

For a GWAS practical, we will use data from the Pan-UKBB project. 

\subsection*{Quantile-Quantile Plots}



\subsection*{Power differential}



\subsection*{}


\end{document}
